{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ab484d70",
      "metadata": {
        "id": "ab484d70"
      },
      "source": [
        "# Laboratorio 9\n",
        "## Ataque y defensa de modelos de Deep Learning\n",
        "Universidad del Valle de Guatemala<br>\n",
        "Security Data Science<br>\n",
        "Pablo Andrés Zamora Vásquez - 21780<br>\n",
        "Diego Andrés Morales Aquino - 21762<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f65ac35a",
      "metadata": {
        "id": "f65ac35a"
      },
      "source": [
        "## Primera parte: Ataques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c6655534",
      "metadata": {
        "id": "c6655534"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from art.estimators.classification import TensorFlowV2Classifier\n",
        "from art.attacks.evasion import FastGradientMethod\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7178e890",
      "metadata": {
        "id": "7178e890"
      },
      "outputs": [],
      "source": [
        "# Modelo entrenado\n",
        "model = keras.models.load_model(\"malware_classification_model.keras\")\n",
        "\n",
        "# Recompilar con eager mode habilitado\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        "    run_eagerly=True  # Necesario para ART con TF2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "47632128",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47632128",
        "outputId": "7f4c0f87-dd9a-443d-af5f-2f3429e3f572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2790 images belonging to 25 classes.\n"
          ]
        }
      ],
      "source": [
        "# Cargar datos\n",
        "dataset_path = \"malimg_paper_dataset_imgs/malimg_paper_dataset_imgs\"\n",
        "img_height, img_width = 64, 64\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1.0 / 255, validation_split=0.3)\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=False,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "X_test, y_test = next(val_generator)\n",
        "for _ in range(len(val_generator) - 1):\n",
        "    x, y = next(val_generator)\n",
        "    X_test = np.concatenate((X_test, x))\n",
        "    y_test = np.concatenate((y_test, y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8423c4bc",
      "metadata": {
        "id": "8423c4bc"
      },
      "outputs": [],
      "source": [
        "# Envolver modelo con ART\n",
        "\n",
        "classifier = TensorFlowV2Classifier(\n",
        "    model=model,\n",
        "    nb_classes=25,\n",
        "    input_shape=(64, 64, 3),\n",
        "    loss_object=tf.keras.losses.CategoricalCrossentropy(),\n",
        "    clip_values=(0.0, 1.0)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "146900bc",
      "metadata": {
        "id": "146900bc"
      },
      "source": [
        "### Ataque #1: FGSM (Evasión)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7fd426c5",
      "metadata": {
        "id": "7fd426c5"
      },
      "outputs": [],
      "source": [
        "# Ataque FGSM\n",
        "attack = FastGradientMethod(estimator=classifier, eps=0.1)\n",
        "X_adv = attack.generate(x=X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "704f634b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "704f634b",
        "outputId": "b66f2e7d-479c-4905-c05b-f3b0941a82de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy original: 0.9594982078853047\n",
            "Accuracy con ataque FGSM: 0.22903225806451613\n"
          ]
        }
      ],
      "source": [
        "# Evaluación\n",
        "preds_original = np.argmax(classifier.predict(X_test), axis=1)\n",
        "preds_adv = np.argmax(classifier.predict(X_adv), axis=1)\n",
        "true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"Accuracy original:\", accuracy_score(true_labels, preds_original))\n",
        "print(\"Accuracy con ataque FGSM:\", accuracy_score(true_labels, preds_adv))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "888ec2c7",
      "metadata": {
        "id": "888ec2c7"
      },
      "source": [
        "Este primer ataque consiste en generar ejemplos engañosos utilizando los gradientes de la red neuronal. Para ello, se calculan los gradientes de la función de pérdida con respecto a la imagen de entrada, y se utiliza esta información para crear una nueva imagen que maximice la pérdida. El objetivo es lograr que el modelo clasifique erróneamente estas entradas alteradas.\n",
        "\n",
        "El parámetro eps de 0.1 (magnitud de perturbación) indica que se realizan cambios sutiles pero suficientes para confundir al modelo.\n",
        "\n",
        "Al evaluar las imágenes modificadas para maximizar la pérdida del modelo, se puede observar que la precisión disminuye considerablemente con respecto a los datos originales. Esto evidencia la vulnerabilidad del modelo frente a perturbaciones diseñadas específicamente para engañarlo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba0d5372",
      "metadata": {
        "id": "ba0d5372"
      },
      "source": [
        "### Ataque #2: Black-box (Inferencia)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ccd33e78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccd33e78",
        "outputId": "7c1a15cf-a70b-40d6-859b-72ccb98d62aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6549 images belonging to 25 classes.\n"
          ]
        }
      ],
      "source": [
        "# Reutilizar el mismo ImageDataGenerator pero con subset=\"training\"\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=False,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "# Convertir train_generator en arrays\n",
        "X_train, y_train = next(train_generator)\n",
        "for _ in range(len(train_generator) - 1):\n",
        "    x, y = next(train_generator)\n",
        "    X_train = np.concatenate((X_train, x))\n",
        "    y_train = np.concatenate((y_train, y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a237dacd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a237dacd",
        "outputId": "ce2e4eb0-8956-45db-b6e6-ace7e7e29ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TPR (miembros):    1.000\n",
            "FPR (no-miembros): 0.513\n",
            "Advantage:      0.487\n"
          ]
        }
      ],
      "source": [
        "from art.attacks.inference.membership_inference import MembershipInferenceBlackBox\n",
        "import numpy as np\n",
        "\n",
        "# 1) Balancear tamaños\n",
        "min_len = min(len(X_train), len(X_test))\n",
        "X_train_bal, y_train_bal = X_train[:min_len], y_train[:min_len]\n",
        "X_test_bal,  y_test_bal  = X_test[:min_len],  y_test[:min_len]\n",
        "\n",
        "# 2) Instanciar el ataque\n",
        "mi_attack = MembershipInferenceBlackBox(\n",
        "    estimator=classifier,\n",
        "    input_type=\"prediction\",      # usa directamente las probabilidades del modelo\n",
        "    attack_model_type=\"nn\",\n",
        "    scaler_type=\"minmax\",         # normaliza las features antes de entrenar el adversario\n",
        "    nn_model_epochs=50,\n",
        "    nn_model_batch_size=32\n",
        ")\n",
        "\n",
        "# 3) Entrenar el ataque\n",
        "mi_attack.fit(\n",
        "    x=X_train_bal,\n",
        "    y=y_train_bal,\n",
        "    test_x=X_test_bal,\n",
        "    test_y=y_test_bal\n",
        ")\n",
        "\n",
        "# 4) Inferir membership (pasando siempre las etiquetas)\n",
        "pred_train = mi_attack.infer(x=X_train_bal, y=y_train_bal)\n",
        "pred_test  = mi_attack.infer(x=X_test_bal,  y=y_test_bal)\n",
        "\n",
        "# 5) Métricas\n",
        "tpr       = np.mean(pred_train == 1)\n",
        "fpr       = np.mean(pred_test  == 1)\n",
        "advantage = tpr - fpr\n",
        "\n",
        "print(f\"TPR (miembros):    {tpr:.3f}\")\n",
        "print(f\"FPR (no-miembros): {fpr:.3f}\")\n",
        "print(f\"Advantage:      {advantage:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8effc149",
      "metadata": {
        "id": "8effc149"
      },
      "source": [
        "Como segundo tipo, se tiene un ataque de inferencia de membresía tipo black box. Este busca determinar si ciertas imágenes fueron utilizadas durante el entrenemiento del modelo. Para el ataque se utiliza \"MembershipInferenceBlackBox\", indicándole que solo se debe tener acceso a las probabilidades de predicción y no a los datos internos. Al entrenar el modelo, se le pasan tanto ejemplos positivos (los datos que el modelo sí vio durante el entrenamiento) y datos negativos (imágenes que el modelo no vio). Al ejecutar la función infer sobre el modelo atacante, este intenta adivinar si cada imagen pertenece o no al conjunto de entrenamiento original. Los ejemplos positivos deberían de dar valores cercanos a 1, indicando que fueron identificados como miembros. Mientras que, los ejemplos negativos deberían ser cercanos a 0, indicando que no son miembros.\n",
        "\n",
        "La métrica TPR representan la proporción de imágenes que sí pertenecen al conjunto de entrenamiento (miembros) correctamente identificadas. Mientras que, FPR es la proporción de no miembros incorrectamente clasificados como miembros. Advantage mide qué tanto mejor que al azar lo está haciendo el atacante. En este caso, el atacante identificó correctamente el 100% de los ejemplos que sí estaban en el conjunto de entrenamiento. Así como, clasificó erróneamente como miembros a más del 51% de los ejemplos que no estaban en el entrenamiento. Sin embargo, el advantage de 0.487 indica que se puede predecir la pertenencia al conjunto de entrenamiento casi un 49% mejor que el azar, lo cual refleja una fuga de información.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d22b775",
      "metadata": {
        "id": "5d22b775"
      },
      "source": [
        "## Segunda parte: Defensas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e40ea99b",
      "metadata": {
        "id": "e40ea99b"
      },
      "source": [
        "### Defensa #1: Adversarial Training\n",
        "\n",
        "Se reentrena el modelo incluyendo ejemplos FGSM para que aprenda a ser robusto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b065d589",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b065d589",
        "outputId": "f75e472e-c522-4fb7-db55-b54e873f3ef0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 392ms/step - accuracy: 0.4598 - loss: 1.8181 - val_accuracy: 0.4416 - val_loss: 1.5231\n",
            "Epoch 2/10\n",
            "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 393ms/step - accuracy: 0.7874 - loss: 0.6512 - val_accuracy: 0.5599 - val_loss: 1.5223\n",
            "Epoch 3/10\n",
            "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 381ms/step - accuracy: 0.8975 - loss: 0.3616 - val_accuracy: 0.5408 - val_loss: 2.1487\n",
            "Epoch 4/10\n",
            "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 393ms/step - accuracy: 0.9298 - loss: 0.2376 - val_accuracy: 0.5920 - val_loss: 1.6056\n",
            "Epoch 5/10\n",
            "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 399ms/step - accuracy: 0.9421 - loss: 0.1752 - val_accuracy: 0.5798 - val_loss: 1.9356\n",
            "Epoch 6/10\n",
            "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 395ms/step - accuracy: 0.9524 - loss: 0.1428 - val_accuracy: 0.5756 - val_loss: 2.4433\n",
            "Epoch 7/10\n",
            "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 381ms/step - accuracy: 0.9567 - loss: 0.1335 - val_accuracy: 0.5798 - val_loss: 1.9578\n",
            "Epoch 8/10\n",
            "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 391ms/step - accuracy: 0.9657 - loss: 0.1086 - val_accuracy: 0.5786 - val_loss: 2.6767\n",
            "Epoch 9/10\n",
            "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 377ms/step - accuracy: 0.9644 - loss: 0.1050 - val_accuracy: 0.6137 - val_loss: 2.2097\n",
            "Epoch 10/10\n",
            "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 390ms/step - accuracy: 0.9692 - loss: 0.0925 - val_accuracy: 0.6416 - val_loss: 2.5206\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 80ms/step\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 76ms/step\n",
            "Accuracy FGSM antes de defensa: 0.229\n",
            "Accuracy FGSM tras adversarial training: 0.835\n"
          ]
        }
      ],
      "source": [
        "# Generar adversariales sobre tu train set\n",
        "fgsm = FastGradientMethod(estimator=classifier, eps=0.1)\n",
        "X_train_adv = fgsm.generate(x=X_train)\n",
        "y_train_adv = y_train.copy()\n",
        "\n",
        "#Combinar datos limpios + adversariales\n",
        "X_comb = np.concatenate([X_train, X_train_adv], axis=0)\n",
        "y_comb = np.concatenate([y_train, y_train_adv], axis=0)\n",
        "\n",
        "# Clonar y recompilar tu modelo base\n",
        "model_def = keras.models.clone_model(model)\n",
        "model_def.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        "    run_eagerly=True\n",
        ")\n",
        "\n",
        "# Reentrenar sobre el conjunto combinado\n",
        "model_def.fit(\n",
        "    X_comb, y_comb,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Envolver el modelo defendido\n",
        "classifier_def = TensorFlowV2Classifier(\n",
        "    model=model_def,\n",
        "    nb_classes=25,\n",
        "    input_shape=(64,64,3),\n",
        "    loss_object=tf.keras.losses.CategoricalCrossentropy(),\n",
        "    clip_values=(0.0,1.0)\n",
        ")\n",
        "\n",
        "# Evaluar defensa ante FGSM\n",
        "X_adv_test = fgsm.generate(x=X_test)\n",
        "acc_before = np.mean(np.argmax(model.predict(X_adv_test),axis=1) == np.argmax(y_test,axis=1))\n",
        "acc_after  = np.mean(np.argmax(model_def.predict(X_adv_test),axis=1) == np.argmax(y_test,axis=1))\n",
        "print(f\"Accuracy FGSM antes de defensa: {acc_before:.3f}\")\n",
        "print(f\"Accuracy FGSM tras adversarial training: {acc_after:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcdef12c",
      "metadata": {
        "id": "dcdef12c"
      },
      "source": [
        "Esta defensa consiste en fortalecer el modelo al exponerlo a ejemplos adversariales durante su entrenamiento. Para ello primero se generan imágenes perturbadas del conjunto de entrenamiento, combinándolo con los datos originales para formar un nuevo conjunto de entrenamiento más robusto.\n",
        "\n",
        "Bajo ataque FGSM el modelo original solo acertaba el 23.2 % de las imágenes adversariales. Tras reentrenar con ejemplos FGSM, el mismo adversario logra solo un 83.5 % de acierto, recuperando robustez y subiendo la accuracy en adversariales desde 0.23 hasta 0.84. Esto significa que adversarial training es muy efectivo atacando directamente la perturbación FGSM, pues el modelo aprendió a reconocer y resistir esas pequeñas modificaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0169a9f",
      "metadata": {
        "id": "d0169a9f"
      },
      "source": [
        "### Defensa 2: Feature Squeezing\n",
        "\n",
        "Aplicar un preprocesamiento que reduce la cantidad de “bits” de información de cada píxel, haciendo más difícil ocultar perturbaciones pequeñas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4a82b4f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a82b4f1",
        "outputId": "212a9901-24d2-4c40-902c-b31dc438c9b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 77ms/step\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 92ms/step\n",
            "Accuracy con Feature Squeezing (modelo original): 0.374\n",
            "Accuracy con Feature Squeezing (modelo defendido): 0.935\n"
          ]
        }
      ],
      "source": [
        "from art.defences.preprocessor import FeatureSqueezing\n",
        "\n",
        "# 1) Crear el preprocesador\n",
        "squeezer = FeatureSqueezing(\n",
        "    clip_values=(0.0, 1.0),\n",
        "    bit_depth=2\n",
        ")\n",
        "\n",
        "# 2) Aplicar el preprocesador a los adversariales\n",
        "X_squeezed, _ = squeezer(X_adv_test, y=y_test)\n",
        "\n",
        "# 3) Evaluar\n",
        "acc_clean    = np.mean(\n",
        "    np.argmax(model.predict(X_squeezed),    axis=1)\n",
        "    == np.argmax(y_test, axis=1)\n",
        ")\n",
        "acc_defended = np.mean(\n",
        "    np.argmax(model_def.predict(X_squeezed), axis=1)\n",
        "    == np.argmax(y_test, axis=1)\n",
        ")\n",
        "\n",
        "print(f\"Accuracy con Feature Squeezing (modelo original): {acc_clean:.3f}\")\n",
        "print(f\"Accuracy con Feature Squeezing (modelo defendido): {acc_defended:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a029173",
      "metadata": {
        "id": "8a029173"
      },
      "source": [
        "En esta defensa se está reduciendo la precisión de las características de entrada para hacer más dificil que ataques adversariales engañen al modelo.\n",
        "\n",
        "Aplicando el preprocesador de bit-depth=2 sobre el modelo sin retraining, la accuracy ante FGSM sube del 23.2% a 38.3%. Si se combina el squeezing con el modelo adversarialmente entrenado, la accuracy salta hasta 93.5 % sobre esos mismos ejemplos adversariales. Esto indica que Feature Squeezing, al reducir la información por píxel, ya aporta cierta protección al modelo original y refuerza aún más al modelo entrenado con adversariales."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jy0NRHtWldR_",
      "metadata": {
        "id": "Jy0NRHtWldR_"
      },
      "source": [
        "### Defensa de ataque 2: FeatureSqueezing y JpegCompression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "-CWeqdu5B9OQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CWeqdu5B9OQ",
        "outputId": "61c50835-73f5-4881-9eb2-162e28ad1df8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 395ms/step - accuracy: 0.4162 - loss: 1.8523 - val_accuracy: 0.0236 - val_loss: 10.8635\n",
            "Epoch 2/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 393ms/step - accuracy: 0.7404 - loss: 0.6225 - val_accuracy: 0.0253 - val_loss: 16.5526\n",
            "Epoch 3/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 384ms/step - accuracy: 0.8711 - loss: 0.3434 - val_accuracy: 0.0253 - val_loss: 20.8891\n",
            "Epoch 4/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 390ms/step - accuracy: 0.9535 - loss: 0.1425 - val_accuracy: 0.0253 - val_loss: 22.2467\n",
            "Epoch 5/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 398ms/step - accuracy: 0.9423 - loss: 0.1733 - val_accuracy: 0.0253 - val_loss: 25.9724\n",
            "Epoch 6/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 377ms/step - accuracy: 0.9744 - loss: 0.0940 - val_accuracy: 0.0253 - val_loss: 23.0743\n",
            "Epoch 7/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 393ms/step - accuracy: 0.9789 - loss: 0.0644 - val_accuracy: 0.0253 - val_loss: 26.1945\n",
            "Epoch 8/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 399ms/step - accuracy: 0.9709 - loss: 0.1045 - val_accuracy: 0.0253 - val_loss: 27.7403\n",
            "Epoch 9/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 399ms/step - accuracy: 0.9782 - loss: 0.0630 - val_accuracy: 0.0253 - val_loss: 22.2327\n",
            "Epoch 10/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 390ms/step - accuracy: 0.9784 - loss: 0.0634 - val_accuracy: 0.0253 - val_loss: 25.9404\n",
            "\n",
            "--- Con JpegCompression + FeatureSqueezing ---\n",
            "TPR (miembros):    0.984\n",
            "FPR (no-miembros): 0.861\n",
            "Advantage:         0.123\n"
          ]
        }
      ],
      "source": [
        "from art.defences.preprocessor import FeatureSqueezing, JpegCompression\n",
        "from art.estimators.classification import TensorFlowV2Classifier\n",
        "from art.attacks.inference.membership_inference import MembershipInferenceBlackBox\n",
        "\n",
        "jpeg_compressor = JpegCompression(clip_values=(0.0, 1.0), quality=75)\n",
        "fsq = FeatureSqueezing(clip_values=(0.0, 1.0), bit_depth=10)\n",
        "\n",
        "# Aplicar ambos preprocesamientos secuencialmente a los datos\n",
        "X_train_def, _ = jpeg_compressor(X_train)\n",
        "X_train_def, _ = fsq(X_train_def)\n",
        "\n",
        "X_test_def, _ = jpeg_compressor(X_test)\n",
        "X_test_def, _ = fsq(X_test_def)\n",
        "\n",
        "# Clonar y recompilar modelo\n",
        "model_def = keras.models.clone_model(model)\n",
        "model_def.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        "    run_eagerly=True\n",
        ")\n",
        "\n",
        "# Reentrenar el modelo con datos defendidos\n",
        "model_def.fit(\n",
        "    X_train_def, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Envolver el modelo con ART\n",
        "classifier_def = TensorFlowV2Classifier(\n",
        "    model=model_def,\n",
        "    nb_classes=25,\n",
        "    input_shape=(64, 64, 3),\n",
        "    loss_object=tf.keras.losses.CategoricalCrossentropy(),\n",
        "    clip_values=(0.0, 1.0)\n",
        ")\n",
        "\n",
        "# Ataque de inferencia de membresía\n",
        "mi_attack_def = MembershipInferenceBlackBox(\n",
        "    estimator=classifier_def,\n",
        "    input_type=\"prediction\",\n",
        "    attack_model_type=\"nn\",\n",
        "    scaler_type=\"minmax\",\n",
        "    nn_model_epochs=50,\n",
        "    nn_model_batch_size=32\n",
        ")\n",
        "\n",
        "# Entrenar al atacante con datos defendidos\n",
        "mi_attack_def.fit(x=X_train_def, y=y_train, test_x=X_test_def, test_y=y_test)\n",
        "\n",
        "# Inferencia del atacante\n",
        "pred_train_def = mi_attack_def.infer(x=X_train_def, y=y_train)\n",
        "pred_test_def  = mi_attack_def.infer(x=X_test_def,  y=y_test)\n",
        "\n",
        "tpr_def = np.mean(pred_train_def == 1)\n",
        "fpr_def = np.mean(pred_test_def == 1)\n",
        "advantage_def = tpr_def - fpr_def\n",
        "\n",
        "print(\"\\n--- Con JpegCompression + FeatureSqueezing ---\")\n",
        "print(f\"TPR (miembros):    {tpr_def:.3f}\")\n",
        "print(f\"FPR (no-miembros): {fpr_def:.3f}\")\n",
        "print(f\"Advantage:         {advantage_def:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PNzByfjRjgXm",
      "metadata": {
        "id": "PNzByfjRjgXm"
      },
      "source": [
        "La defensa del modelo consiste en aplicar técnicas de preprocesamiento, como la compresión JPEG y el \"Feature Squeezing\", para proteger el modelo de ataques de inferencia de membresía. En primer lugar, se preprocesan los datos de entrenamiento y test utilizando compresión JPEG para reducir la calidad de la imagen, mientras que se usa \"Feature Squeezing\" para reducir la profundidad de bits de las características.\n",
        "\n",
        "Los resultados con la defensa muestran una reducción en la efectividad del ataque. Aunque el TPR disminuye ligeramente de 1.000 a 0.984, el FPR aumenta considerablemente de 0.513 a 0.861, lo que indica que el atacante comete más falsos positivos. Esto provoca una disminución en el Advantage, que pasa de 0.487 sin defensa a 0.123 con defensa. Aunque esta mejora sugiere que el modelo tiene un mayor grado de protección, no se lograron resultados óptimos, ya que aún es capaz de identificar correctamente una gran cantidad de imágenes. Además de la combinación de defensas, se probaron ambas técnicas de manera individual, obteniendo resultados algo inferiores. Por otro lado, se intentó aplicar LabelSmoothing, pero este enfoque favoreció al ataque, permitiendo que se identificara correctamente al 100% de los miembros sin generar falsos positivos."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
