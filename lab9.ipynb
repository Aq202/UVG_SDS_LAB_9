{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab484d70",
   "metadata": {},
   "source": [
    "# Laboratorio 9\n",
    "## Ataque y defensa de modelos de Deep Learning\n",
    "Universidad del Valle de Guatemala<br>\n",
    "Security Data Science<br>\n",
    "Pablo Andrés Zamora Vásquez - 21780<br>\n",
    "Diego Andrés Morales Aquino - 21762<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ac35a",
   "metadata": {},
   "source": [
    "## Primera parte: Ataques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6655534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7178e890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pablo Zamora\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 12 variables whereas the saved optimizer has 22 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# Modelo entrenado\n",
    "model = keras.models.load_model(\"malware_classification_model.keras\")\n",
    "\n",
    "# Recompilar con eager mode habilitado\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    run_eagerly=True  # Necesario para ART con TF2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47632128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2790 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "dataset_path = \"malimg_paper_dataset_imgs\"\n",
    "img_height, img_width = 64, 64\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1.0 / 255, validation_split=0.3)\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "X_test, y_test = next(val_generator)\n",
    "for _ in range(len(val_generator) - 1):\n",
    "    x, y = next(val_generator)\n",
    "    X_test = np.concatenate((X_test, x))\n",
    "    y_test = np.concatenate((y_test, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8423c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Envolver modelo con ART\n",
    "\n",
    "classifier = TensorFlowV2Classifier(\n",
    "    model=model,\n",
    "    nb_classes=25,\n",
    "    input_shape=(64, 64, 3),\n",
    "    loss_object=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    clip_values=(0.0, 1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146900bc",
   "metadata": {},
   "source": [
    "### Ataque #1: FGSM (Evasión)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd426c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ataque FGSM\n",
    "attack = FastGradientMethod(estimator=classifier, eps=0.1)\n",
    "X_adv = attack.generate(x=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f634b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy original: 0.9594982078853047\n",
      "Accuracy con ataque FGSM: 0.23225806451612904\n"
     ]
    }
   ],
   "source": [
    "# Evaluación\n",
    "preds_original = np.argmax(classifier.predict(X_test), axis=1)\n",
    "preds_adv = np.argmax(classifier.predict(X_adv), axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"Accuracy original:\", accuracy_score(true_labels, preds_original))\n",
    "print(\"Accuracy con ataque FGSM:\", accuracy_score(true_labels, preds_adv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0d5372",
   "metadata": {},
   "source": [
    "### Ataque #2: Black-box (Inferencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccd33e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6549 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "# Reutilizar el mismo ImageDataGenerator pero con subset=\"training\"\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=False,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Convertir train_generator en arrays\n",
    "X_train, y_train = next(train_generator)\n",
    "for _ in range(len(train_generator) - 1):\n",
    "    x, y = next(train_generator)\n",
    "    X_train = np.concatenate((X_train, x))\n",
    "    y_train = np.concatenate((y_train, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237dacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR (miembros):    1.000\n",
      "FPR (no-miembros): 0.513\n",
      "Advantage:      0.487\n"
     ]
    }
   ],
   "source": [
    "from art.attacks.inference.membership_inference import MembershipInferenceBlackBox\n",
    "import numpy as np\n",
    "\n",
    "# 1) Balancear tamaños\n",
    "min_len = min(len(X_train), len(X_test))\n",
    "X_train_bal, y_train_bal = X_train[:min_len], y_train[:min_len]\n",
    "X_test_bal,  y_test_bal  = X_test[:min_len],  y_test[:min_len]\n",
    "\n",
    "# 2) Instanciar el ataque\n",
    "mi_attack = MembershipInferenceBlackBox(\n",
    "    estimator=classifier,\n",
    "    input_type=\"prediction\",      # usa directamente las probabilidades del modelo\n",
    "    attack_model_type=\"nn\",       \n",
    "    scaler_type=\"minmax\",         # normaliza las features antes de entrenar el adversario\n",
    "    nn_model_epochs=50,           \n",
    "    nn_model_batch_size=32\n",
    ")\n",
    "\n",
    "# 3) Entrenar el ataque\n",
    "mi_attack.fit(\n",
    "    x=X_train_bal,\n",
    "    y=y_train_bal,\n",
    "    test_x=X_test_bal,\n",
    "    test_y=y_test_bal\n",
    ")\n",
    "\n",
    "# 4) Inferir membership (pasando siempre las etiquetas)\n",
    "pred_train = mi_attack.infer(x=X_train_bal, y=y_train_bal)\n",
    "pred_test  = mi_attack.infer(x=X_test_bal,  y=y_test_bal)\n",
    "\n",
    "# 5) Métricas\n",
    "tpr       = np.mean(pred_train == 1)\n",
    "fpr       = np.mean(pred_test  == 1)\n",
    "advantage = tpr - fpr\n",
    "\n",
    "print(f\"TPR (miembros):    {tpr:.3f}\")\n",
    "print(f\"FPR (no-miembros): {fpr:.3f}\")\n",
    "print(f\"Advantage:      {advantage:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d22b775",
   "metadata": {},
   "source": [
    "## Segunda parte: Defensas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40ea99b",
   "metadata": {},
   "source": [
    "### Defensa #1: Adversarial Training\n",
    "\n",
    "Se reentrena el modelo incluyendo ejemplos FGSM para que aprenda a ser robusto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b065d589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 161ms/step - accuracy: 0.4392 - loss: 1.8605 - val_accuracy: 0.4653 - val_loss: 1.2454\n",
      "Epoch 2/10\n",
      "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 159ms/step - accuracy: 0.7892 - loss: 0.6568 - val_accuracy: 0.5302 - val_loss: 1.2035\n",
      "Epoch 3/10\n",
      "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 162ms/step - accuracy: 0.8907 - loss: 0.3808 - val_accuracy: 0.5504 - val_loss: 1.2297\n",
      "Epoch 4/10\n",
      "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 164ms/step - accuracy: 0.9237 - loss: 0.2480 - val_accuracy: 0.5553 - val_loss: 1.4057\n",
      "Epoch 5/10\n",
      "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 166ms/step - accuracy: 0.9321 - loss: 0.2053 - val_accuracy: 0.5588 - val_loss: 1.7329\n",
      "Epoch 6/10\n",
      "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 161ms/step - accuracy: 0.9433 - loss: 0.1683 - val_accuracy: 0.5553 - val_loss: 1.3384\n",
      "Epoch 7/10\n",
      "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 160ms/step - accuracy: 0.9425 - loss: 0.1669 - val_accuracy: 0.5450 - val_loss: 1.8970\n",
      "Epoch 8/10\n",
      "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 161ms/step - accuracy: 0.9528 - loss: 0.1341 - val_accuracy: 0.6153 - val_loss: 1.8048\n",
      "Epoch 9/10\n",
      "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 161ms/step - accuracy: 0.9545 - loss: 0.1271 - val_accuracy: 0.5557 - val_loss: 2.1876\n",
      "Epoch 10/10\n",
      "\u001b[1m328/328\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 161ms/step - accuracy: 0.9564 - loss: 0.1171 - val_accuracy: 0.6378 - val_loss: 1.5758\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step\n",
      "Accuracy FGSM antes de defensa: 0.232\n",
      "Accuracy FGSM tras adversarial training: 0.835\n"
     ]
    }
   ],
   "source": [
    "# Generar adversariales sobre tu train set\n",
    "fgsm = FastGradientMethod(estimator=classifier, eps=0.1)\n",
    "X_train_adv = fgsm.generate(x=X_train)\n",
    "y_train_adv = y_train.copy()\n",
    "\n",
    "#Combinar datos limpios + adversariales\n",
    "X_comb = np.concatenate([X_train, X_train_adv], axis=0)\n",
    "y_comb = np.concatenate([y_train, y_train_adv], axis=0)\n",
    "\n",
    "# Clonar y recompilar tu modelo base\n",
    "model_def = keras.models.clone_model(model)\n",
    "model_def.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    run_eagerly=True\n",
    ")\n",
    "\n",
    "# Reentrenar sobre el conjunto combinado\n",
    "model_def.fit(\n",
    "    X_comb, y_comb,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Envolver el modelo defendido\n",
    "classifier_def = TensorFlowV2Classifier(\n",
    "    model=model_def,\n",
    "    nb_classes=25,\n",
    "    input_shape=(64,64,3),\n",
    "    loss_object=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    clip_values=(0.0,1.0)\n",
    ")\n",
    "\n",
    "# Evaluar defensa ante FGSM\n",
    "X_adv_test = fgsm.generate(x=X_test)\n",
    "acc_before = np.mean(np.argmax(model.predict(X_adv_test),axis=1) == np.argmax(y_test,axis=1))\n",
    "acc_after  = np.mean(np.argmax(model_def.predict(X_adv_test),axis=1) == np.argmax(y_test,axis=1))\n",
    "print(f\"Accuracy FGSM antes de defensa: {acc_before:.3f}\")\n",
    "print(f\"Accuracy FGSM tras adversarial training: {acc_after:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdef12c",
   "metadata": {},
   "source": [
    "Bajo ataque FGSM el modelo original solo acertaba el 23.2 % de las imágenes adversariales. Tras reentrenar con ejemplos FGSM, el mismo adversario logra solo un 83.5 % de acierto, recuperando robustez y subiendo la accuracy en adversariales desde 0.23 hasta 0.84. Esto significa que adversarial training es muy efectivo atacando directamente la perturbación FGSM, pues el modelo aprendió a reconocer y resistir esas pequeñas modificaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0169a9f",
   "metadata": {},
   "source": [
    "### Defensa 2: Feature Squeezing\n",
    "\n",
    "Aplicar un preprocesamiento que reduce la cantidad de “bits” de información de cada píxel, haciendo más difícil ocultar perturbaciones pequeñas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82b4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step\n",
      "Accuracy con Feature Squeezing (modelo original): 0.383\n",
      "Accuracy con Feature Squeezing (modelo defendido): 0.935\n"
     ]
    }
   ],
   "source": [
    "from art.defences.preprocessor import FeatureSqueezing\n",
    "\n",
    "# 1) Crear el preprocesador\n",
    "squeezer = FeatureSqueezing(\n",
    "    clip_values=(0.0, 1.0),\n",
    "    bit_depth=2\n",
    ")\n",
    "\n",
    "# 2) Aplicar el preprocesador a los adversariales\n",
    "X_squeezed, _ = squeezer(X_adv_test, y=y_test)\n",
    "\n",
    "# 3) Evaluar\n",
    "acc_clean    = np.mean(\n",
    "    np.argmax(model.predict(X_squeezed),    axis=1) \n",
    "    == np.argmax(y_test, axis=1)\n",
    ")\n",
    "acc_defended = np.mean(\n",
    "    np.argmax(model_def.predict(X_squeezed), axis=1) \n",
    "    == np.argmax(y_test, axis=1)\n",
    ")\n",
    "\n",
    "print(f\"Accuracy con Feature Squeezing (modelo original): {acc_clean:.3f}\")\n",
    "print(f\"Accuracy con Feature Squeezing (modelo defendido): {acc_defended:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a029173",
   "metadata": {},
   "source": [
    "Aplicando el preprocesador de bit-depth=2 sobre el modelo sin retraining, la accuracy ante FGSM sube del 23.2% a 38.3%. Si se combina el squeezing con el modelo adversarialmente entrenado, la accuracy salta hasta 93.5 % sobre esos mismos ejemplos adversariales. Esto indica que Feature Squeezing, al reducir la información por píxel, ya aporta cierta protección al modelo original y refuerza aún más al modelo entrenado con adversariales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
